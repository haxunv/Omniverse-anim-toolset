# ğŸ¬ è®­ç»ƒä½ è‡ªå·±çš„é•œå¤´å‚æ•°ç”Ÿæˆæ¨¡å‹

## æ¦‚è¿°

é€šè¿‡å¾®è°ƒå°å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºç”Ÿæˆé•œå¤´å‚æ•°çš„ AI æ¨¡å‹ã€‚

**æœ€ç»ˆæ•ˆæœ**ï¼š
- æ¨¡å‹å¤§å°ï¼š~300MB
- æ¨ç†é€Ÿåº¦ï¼šæ¯«ç§’çº§
- å®Œå…¨ç¦»çº¿ï¼šå¯åµŒå…¥æ’ä»¶åˆ†å‘

## è®­ç»ƒæµç¨‹

```
ç¬¬ä¸€æ­¥              ç¬¬äºŒæ­¥              ç¬¬ä¸‰æ­¥              ç¬¬å››æ­¥
ç”Ÿæˆæ•°æ®     â†’     å¾®è°ƒæ¨¡å‹     â†’     åˆå¹¶å¯¼å‡º     â†’     åµŒå…¥æ’ä»¶
(1000æ¡)          (å‡ å°æ—¶)          (ONNXæ ¼å¼)        (å‘å¸ƒ)
```

## ç¯å¢ƒå‡†å¤‡

### ç¡¬ä»¶è¦æ±‚

| é…ç½® | æœ€ä½è¦æ±‚ | æ¨èé…ç½® |
|------|----------|----------|
| GPU | GTX 1060 6GB | RTX 3060 12GB+ |
| å†…å­˜ | 16GB | 32GB |
| ç¡¬ç›˜ | 20GB ç©ºé—² | SSD 50GB |

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# å®‰è£… PyTorchï¼ˆæ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬ï¼‰
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# å®‰è£…è®­ç»ƒä¾èµ–
pip install transformers peft datasets accelerate bitsandbytes
```

## ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆè®­ç»ƒæ•°æ®

### æ–¹å¼ Aï¼šæœ¬åœ°ç”Ÿæˆï¼ˆå¿«é€Ÿï¼Œä¸éœ€è¦ APIï¼‰

```bash
cd omni/anim/drama/toolset/ai/training
python generate_dataset.py --api local --count 1000 --output dataset.json
```

### æ–¹å¼ Bï¼šä½¿ç”¨ LLM ç”Ÿæˆï¼ˆè´¨é‡æ›´é«˜ï¼‰

```bash
# ä½¿ç”¨ç¡…åŸºæµåŠ¨ API
set SILICONFLOW_API_KEY=ä½ çš„Key
python generate_dataset.py --api siliconflow --count 1000 --output dataset.json

# æˆ–ä½¿ç”¨ DeepSeek API
set DEEPSEEK_API_KEY=ä½ çš„Key
python generate_dataset.py --api deepseek --count 1000 --output dataset.json
```

ç”Ÿæˆçš„æ•°æ®æ ¼å¼ï¼š
```json
[
    {
        "input": "ç¯ç»•è§’è‰²çš„å²è¯—é•œå¤´ï¼Œä»ä½è§’åº¦å‡èµ·",
        "output": {
            "shot_name": "Orbit Shot",
            "duration": 8,
            "path": {"type": "orbit", "radius": 4, "height": {"start": 0.3, "end": 3}},
            "constraint": {"type": "look_at"},
            "modifiers": [{"type": "handheld", "intensity": 0.2}]
        }
    }
]
```

## ç¬¬äºŒæ­¥ï¼šå¾®è°ƒæ¨¡å‹

```bash
python train_model.py \
    --dataset dataset.json \
    --output ./camera_shot_model \
    --epochs 3 \
    --batch-size 4
```

### è®­ç»ƒå‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| --base-model | Qwen/Qwen2.5-0.5B-Instruct | åŸºåº§æ¨¡å‹ |
| --epochs | 3 | è®­ç»ƒè½®æ•° |
| --batch-size | 4 | æ‰¹æ¬¡å¤§å°ï¼ˆæ˜¾å­˜ä¸å¤Ÿå°±è°ƒå°ï¼‰ |
| --lr | 2e-4 | å­¦ä¹ ç‡ |

### é¢„è®¡è®­ç»ƒæ—¶é—´

| æ•°æ®é‡ | RTX 3060 | RTX 4090 |
|--------|----------|----------|
| 1000æ¡ | ~1å°æ—¶ | ~15åˆ†é’Ÿ |
| 5000æ¡ | ~5å°æ—¶ | ~1å°æ—¶ |

## ç¬¬ä¸‰æ­¥ï¼šæµ‹è¯•æ¨¡å‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# åŠ è½½æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = PeftModel.from_pretrained(base_model, "./camera_shot_model")
tokenizer = AutoTokenizer.from_pretrained("./camera_shot_model")

# æµ‹è¯•
prompt = "### è¾“å…¥:\nç¯ç»•è§’è‰²çš„å²è¯—é•œå¤´\n\n### è¾“å‡º:\n"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=256)
print(tokenizer.decode(outputs[0]))
```

## ç¬¬å››æ­¥ï¼šå¯¼å‡ºå’Œéƒ¨ç½²

### åˆå¹¶ LoRA æƒé‡

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM

base = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct")
model = PeftModel.from_pretrained(base, "./camera_shot_model")
merged = model.merge_and_unload()
merged.save_pretrained("./camera_shot_model_merged")
```

### é‡åŒ–ï¼ˆå¯é€‰ï¼Œå‡å°ä½“ç§¯ï¼‰

```python
# ä½¿ç”¨ GPTQ æˆ– AWQ é‡åŒ–
# å¯å°†æ¨¡å‹ä» 1GB å‹ç¼©åˆ° ~300MB
```

### å¯¼å‡º ONNXï¼ˆå¯é€‰ï¼‰

```python
# å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œç”¨äºè·¨å¹³å°éƒ¨ç½²
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("./camera_shot_model_merged")
model.export("camera_shot.onnx")
```

## åœ¨æ’ä»¶ä¸­ä½¿ç”¨

è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åœ¨æ’ä»¶ä¸­ä½¿ç”¨ï¼š

```python
from omni.anim.drama.toolset.ai import LocalModelClient

# ä½¿ç”¨æœ¬åœ°è®­ç»ƒçš„æ¨¡å‹
client = LocalModelClient(model_path="./camera_shot_model_merged")
params = client.generate_shot_params("ç¯ç»•é•œå¤´")
```

## å¸¸è§é—®é¢˜

### Q: æ˜¾å­˜ä¸å¤Ÿæ€ä¹ˆåŠï¼Ÿ

1. å‡å° batch_sizeï¼ˆæ”¹ä¸º 1 æˆ– 2ï¼‰
2. ä½¿ç”¨æ›´å°çš„åŸºåº§æ¨¡å‹ï¼ˆå¦‚ Qwen2.5-0.5Bï¼‰
3. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰

### Q: è®­ç»ƒæ•ˆæœä¸å¥½æ€ä¹ˆåŠï¼Ÿ

1. å¢åŠ è®­ç»ƒæ•°æ®é‡ï¼ˆè‡³å°‘ 1000 æ¡ï¼‰
2. ä½¿ç”¨ LLM ç”Ÿæˆæ›´é«˜è´¨é‡çš„æ•°æ®
3. å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆepochsï¼‰
4. æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®

### Q: å¦‚ä½•æé«˜æ¨ç†é€Ÿåº¦ï¼Ÿ

1. ä½¿ç”¨é‡åŒ–ï¼ˆINT4/INT8ï¼‰
2. å¯¼å‡ºä¸º ONNX æ ¼å¼
3. ä½¿ç”¨ llama.cpp éƒ¨ç½²ï¼ˆéœ€è¦è½¬æ¢æ ¼å¼ï¼‰

## æ–‡ä»¶è¯´æ˜

```
training/
â”œâ”€â”€ generate_dataset.py   # ç”Ÿæˆè®­ç»ƒæ•°æ®
â”œâ”€â”€ train_model.py        # å¾®è°ƒæ¨¡å‹
â”œâ”€â”€ README.md             # æœ¬æ–‡æ¡£
â””â”€â”€ dataset.json          # ç”Ÿæˆçš„æ•°æ®ï¼ˆè®­ç»ƒåäº§ç”Ÿï¼‰
```



